# Transformer-py: a Flexible Framework for POS tagging.

[**Data**](#dataset-and-preprocessing) | [**Training**](#run-bert-variants-for-pos-tagging)

The repository works on fine-tuning of the pre-trained Transformer-based models for Parts-of-speech (POS) tagging. We leverage `chtb_0223.gold_conll`, `phoenix_0001.gold_conll`, `pri_0016.gold_conll` and `wsj_1681.gold_conll` annotated file as dataset for fine-tuning. To reproduce the results, follow the steps below.

In the literature, the initial layers are used to encode general, semantic-irrelevant information. The middle layers usually enable them to produce information-rich representations. The latter layers are good at encoding the abstractive and task-oriented semantic representation. We develop a flexible framework to run such experiments. 

* New February 22th, 2021: Data preprocessing and data information.
* New March 8th, 2021: Train the BERT and custom model, dataset loading script.
    
## TO-DO
* Experiments of Linear Probing.
* Experiments of data efficiency.


## Installation

### Python version

* Python >= 3.8

### Environment

Create an environment from file and activate the environment.

```
conda env create -f environment.yaml
conda activate fabian-pinjie
```

If conda fails to create an environment from `environment.yaml`. This may be caused by the platform-specific build constraints in the file. Try to create one by installing the important packages manually. The `environment.yaml` was built in macOS.

**Note**: Running `conda env export > environment.yaml` will include all the 
dependencies conda automatically installed for you. Some dependencies may not work in different platforms.
We suggest you to use the `--from-history` flag to export the packages to the environment setting file.
Make sure `conda` only exports the packages that you've explicitly asked for.

```
conda env export > environment.yaml --from-history
```

## Dataset and Preprocessing

### Dataset concatenation

We use `chtb_0223.gold_conll`, `phoenix_0001.gold_conll`, `pri_0016.gold_conll` and `wsj_1681.gold_conll` as the data for fine-tuning the pre-trained model.
These files are in the `data` folder. We combine them as one file `sample.conll` for preprocessing in the next step.

```
cd data
cat *.gold_conll >> sample.conll
```

In the following steps, you will preprocess the collected file `sample.conll`, then split them into `sample.train`, `sample.dev` and `sample.test`
for building the datasets. You have to change the relative path to `--datasets_name` if you're using a different file directory .


### Preprocessing and Dataset Splitting

The file `sample.conll` contains irrelevant information for training the neural nets.
We only need the sequence of observation, POS tags and the word position for the positional embedding in the transformer. Running `data_preprocess.py` to extract `word position`, `word` and `POS tag` and write it to
`sample.tsv` in which `word position`, `word` and `POS tag` are separated by tab. 

The arguments `--dataset_name` and `output_dir` are the file to be passed to the program and the repository for the output file respectively. 

It generates `sample.tsv` for all examples and `sample.train`, `sample.dev` and `sample.test` for the network training.  The examples will be shuffled in the scripts and split into `train`, `validation` and `test` files.  The arguments `--eval_samples` and `--test_samples`
decide the number of samples will be selected from examples. In OntoNotes datasets, we select 67880 for training set, 2000 for validation and test sets respectively. To preprocess and split the datasets, you need to run the code below. 

```python
python data_preprocess.py \
  --dataset_name sample.conll \
  --output_dir ./ \
  --eval_samples 2000 \
  --test_samples 2000 
```

Or just run the bash script `source ./run_preprocess.sh` in the command line. The output file `sample.tsv` will under the 
path `--output_dir`. You will get the result.

```
Loading 69880 examples
Seed 49 is used to shuffle examples
Saving 69880 examples to sample.tsv
Saving 65880 examples to sample.train
Saving 2000 examples to sample.dev
Saving 2000 examples to sample.test
```

Make sure that **the datasets** you passed to the argument `--dataset_name` has larger number examples for splitting out develop and test set. The example files may have no example, if the splitting number for eval and test sets is greater than the example in `sample.conll`.


### Data Information

To get the information regarding the observations and POS taggings. Execute the script `data_information.py` to compute 
the percentiles, maximum, minimum and mean of the sequence length, number of examples, POS tags and its percentage.

The arguments `--dataset_name` and `output_dir` are the file to be passed to the program and the repository for the output file respectively. 

```python
python data_information.py \
  --dataset_name sample.tsv \
  --output_dir ./
```

or run `source ./run_information.sh` in the command line. The output file `sample.info` will be exported in the  `--output_dir` directory.

### Train with Custom OntoNotes v4.0

We use our dataset loading script `ontonotes_v4.py`for creating dataset. The script builds the train, validation and test sets from those 
dataset splits obtained by the `data_preprocess.py` program. 
Make sure the dataset split files `sample.train`, `sample.dev` , and `sample.test` are included in the datasets folder `data/` your dataset folder.

If you get an error message like:

```
pyarrow.lib.ArrowTypeError: Could not convert 1 with type int: was not a sequence or recognized null for conversion to list type
```

You may have run other datasets in the same folder before. The Huggingface already created `.arrow` files once you run a loading script. These files are for reloading the datasets quickly.

Try to move the dataset you would like to use to the other folder and modify the path in the loading scipt. 

Or delete the relavent folder and files in the `.cache` for datasets. `cd ~/USERS_NAME/.cache/huggingface/datasets/` and `rm -r *`. This means that all the loading records will be removed and
 Hugginface will create the `.arrows` files again, including the previous laoding records. 


## Save the Results

We suggest that using `Weights & Biass` to save the configuration, loss and evaluation metrics for you.
To connect your own `Weights & Biass` account. Just installing the packages using `pip install wandb`
and login it. The `trainer` in `run_pos.py` will automatically log the `TrainingArguments`, losses and evaluation
metrics and model information to your account. 

```
wandb login
```

You can specify which project folder for saving files. For example, set project name 
to the environment variable.

```
export WANDB_PROJECT=TEST_PROJECT
export WANDB_WATCH=all
```

## Run BERT variants for POS tagging

We evaluate the BERT on linear probing test to see which layer capture more linguistic structure 
information in their contextual representations. The output layers for classifying the POS tags are added on the different layers of BERT. We only train these layer's weights.

We treat BERT as a feature extractor to provide fixed pre-trained contextual embeddings.
In the script, we set `requires_grad` false for BERT model. If you would like to fine-tune the whole model, just comment those lines.

In certain cases, rather than fine-tuning the entire pre-trained model end-to-end, it can be beneficial to obtain pre-trained contextual embeddings, which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues.

We found that executing a  64 minibatch size trained with maximal sequence length is pretty slow. 
The maximal sequence length, in OntoNotes is 228, is usually an extreme case. We gain huge improvement on the runtime for a minibatch by using 63 to `max_seq_length` covering 99% of sequence length.


### Train the official BERT model

The official Huggingface BERT for sequence labeling task using `BertForTokenClassification` class. 
The model leverages a pre-trained BERT, dropout and a classifier layer.
To run these settings, you can run

```python
python run_pos.py \
 --model_name_or_path bert-base-cased \
 --output_dir /tmp/pos-exp-1 \
 --task_name pos \
 --dataset_script ontonotes_v4.py \
 --max_seq_length 63 \
 --per_device_train_batch_size 48 \
 --per_device_eval_batch_size 48 \
 --num_train_epochs 3 \
 --do_train \
 --do_eval \
 --do_predict
 --learning_rate 1e-2 
![](data:image/jpeg;base64,{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Phrase-based Machine Translation</center></h1>\n",
    "<h4><center>Pin-Jie Lin</center></h4>\n",
    "<center>pili00001@stud.uni-saarland.de</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IBM Model 1 assumes that each word in the target sentence is a translation of exactly zero or one word of the source sentence. However, the translation quality obtained from word-by-word translation isn't fluent and readable. Additionally, the result does not always cover the necessary meaning from source sentence. In this work, we tackle these problems by leveraging phrase-based translation models. The models estimate the conditional probability built on the extracted phrases from word alignment. We demonstrate that the results from phrase-based model gain the improvements in terms of the fluency and the adequacy than word-based translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to estimate the conditional probability for a phrase-by-phrase translation model. It translates an English sentence and a French sentence. We denote p(e|f) for the conditional probabilistic model. In the works, the source sentence refers to the French sentence that we would like to translate. We also use foreign language or foreign sentences for it. The target language is the English sentence, results of translation obtained from our model.\n",
    "\n",
    "In the programming parts, we always use `source` and `src` for the word,  phrase or sentence in French. On the other hand, `target` and `tgt` are the variable names for words, phrases and sentences in English.\n",
    "\n",
    "This project makes the contributions as follow:\n",
    "- Improve the translation model using **phrase extraction**.\n",
    "- Efficient search possible translated phrase for surce phrase during the phrase translation process.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Phrase-based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second, we now introduce the phrase-based models on noisy-channel assumption, then provide more details on how we estimate the probabilistic model and translate a sentence using dencoding algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical Machine Translation.** We define P(e|f) as a conditional probabilistic model of a French sentence **f** \n",
    "given an English sentence **e**. Mathematically, we can apply Bayes rule to derive the formula, which is known as **noisy-channel model**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "P(e|f) \\propto P(f|e)P(e)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the conditional probabilistic model **P(e|f)** is proportional to a translation model **P(f|e)** and a language model **P(e)**. In the word-based model, this translation model **P(f|e)** can be seen as the summation of all probabilities for the alignmens between French and English sentences. In practice, it is infeasible to compute all possible alignments. The word-based translation models approximate **P(f|e)** with an assumpition that each word in the French sentence is a translation of exactly zero or one word of the English sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "P(f|e) & = \\sum_{a} P(f,a|e) \\\\\n",
    "& \\propto \\prod_{j=1}^{l_{f}} \\sum_{i=1}^{l_{e}} P(f_{j}|e_{i}) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, such word-based model uses only lexical translation probabilities and not sufficient to translate a sentence. On the other hand, the phrased-based model rewrites the translation model **p(f|e)** as the product of phrase translation probability $\\phi(f_{i}|e_{i})$ and distance-based reordering model $d(start_{i}-end_{i-1}-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "P_{phrase}(f|e) = \\prod_{i=1}^{I} \\phi_(f_{i}|e_{i}) d(start_{i}-end_{i-1}-1)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log-probability Form** In the implementation, the program finds the most probable English translation such that it maximize the formula. To avoid overflow, we replace the prodct with log-probability: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "e* & = argmax_{e} P(e|f) \\\\\n",
    "& = arg max_{e} P_{phrase}(f|e) \\times P_{LM}(e) \\\\\n",
    "& = arg max_{e} log P_{phrase}(f|e) + log P_{LM}(e) \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phrase Extraction.** The phrase-based model builds a phrase table on the word alignments. We get the word aligment from our IBM Model implemetation running on 100k `Hansards` French-English datasests. \n",
    "\n",
    "\n",
    "There are two steps to extract the possible phrases from word alignment. First, loop all possible phrases in German matching the minimal phrase in English. Second, find the shortest phrase in English that includes all the enterparts for the German words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoding** To obtain the best translated sentence from a foreign input sentence, our phrase-based model computes scores for partial translations in the decoding step. In the decoding program, the partial translations are a stacked phrase called hypothesis. We uses a heurstic algorithm called **beam-search**. The beam-search decoding algorithm keeps a **k** fixed number of hypothesise at each time steps. It generates the translation phrase by phrase from left-to-right. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "argmax_{e} log p_{translation}(g, a | e) + log p_{LM}(e) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset.** In order to evalute the Zipf's law, we select four corpus to see whether it is true for different domains of text and languages. First, **the King James Bible (KJB)**  is an English translation of the Christian Bible which consists of 31102 sentences. Second, **the Jungle Book** is a collection of stories which has roughly 54887 words counting from segments. For last two corpora, we use parallel news texts, **SETimes Turkish-Bulgarian corpus**, which base on the SETimes.com news portal and consist 206071 parallel sentences in both Turkish and Bulgarian language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We uses **Hansards** Canadian-English datasets. The datasets dervied from our word alignment assignment. It consists of 100k parallel sentences in French and English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Experiements\n",
    "\n",
    "In this section, we briefly introduce our datasets: the King James Bible, the Jungle Book and the SETimesTurkish-Bulgarian parallel newspaper text. In addition, we descript our implemation detail here.\n",
    "\n",
    "**Experimental settings.** We uses two scripts for building the data and ploting the Zipf's law charts separately. For each corpus, we count its word frequency and store word-frequency pair as kay-value mapping in a dictionary sorting by descending order. We tokenize sentence by whitespace and remain all stop words and punctuation remarks. Because it does not effect the result of zipf's law. In implementation, we collect all corpus on a folder and preprocess them within a loop. We dump the word-frequnecy pairs to a `.json` file for each corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Eexperiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we briefly introduce our datasets: the King James Bible, the Jungle Book and the SETimesTurkish-Bulgarian parallel newspaper text. In addition, we descript our implemation detail here.\n",
    "\n",
    "**Experimental settings.** We uses two scripts for building the data and ploting the Zipf's law charts separately. For each corpus, we count its word frequency and store word-frequency pair as kay-value mapping in a dictionary sorting by descending order. We tokenize sentence by whitespace and remain all stop words and punctuation remarks. Because it does not effect the result of zipf's law. In implementation, we collect all corpus on a folder and preprocess them within a loop. We dump the word-frequnecy pairs to a `.json` file for each corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Alignment** Phrase-based translation model extracts all possible phrases from the word-to-word alignments and tained on these file. We obtained the word aligment files by running our IBM model 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation Examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2017 , Beam search strategies for neural machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
)```

### Train Linear Probing BERT  

Linear probing BERT is an architecture to extract the fixed contextual representations from the BERT.
It aims to evaluate which layer captures linguistic structure information in their features.

The custom model is `bert-base-cased`. Therefore, it has one embedding layer in 12 BERT layer in 
BERT model. If you use a classifier on top of 12th BERT's layer. It is same as the standard BERT that 
`BertForTokenClassification` class creats for you. 

To train BERT model on linear probing setting, you have to specify `linear-probing-bert.py` to
the option `--model_name_or_path` and pass integer indicating on which BERT's layer the classifier heads on.

```python
 python run_pos.py \
 --model_name_or_path models/linear-probing-bert.py \
 --output_dir /tmp/pos-exp-1 \
 --task_name pos \
 --dataset_script ontonotes_v4.py \
 --max_seq_length 63 \
 --per_device_train_batch_size 48 \
 --per_device_eval_batch_size 48 \
 --max_steps 120 \
 --do_train \
 --do_eval \
 --do_predict \
 --max_train_samples 10000 \
 --max_val_samples 300 \
 --max_test_samples 300 \
 --logging_first_step \
 --logging_steps 5 \
 --learning_rate 1e-2 \
 --evaluation_strategy steps \
 --eval_steps 10 \
 --to_layer 2 
```


### Train your custom model

Run the custom model via the path `models/custom-model-demo.py`.
You can define your custom model by modifying the demo script.

```python
python run_pos.py \
 --model_name_or_path models/custom-model-demo.py \
 --output_dir /tmp/pos-exp-1 \
 --task_name pos \
 --dataset_script ontonotes_v4.py \
 --max_seq_length 63 \
 --per_device_train_batch_size 16 \
 --per_device_eval_batch_size 8 \
 --num_train_epochs 3 \
 --do_train \
 --do_eval \
 --do_predict
  --learning_rate 1e-2 \
```

### Quicker training  

If you'd like to further develop the model or debugging it. 
The options `max_train_samples`, `max_vall_samples` and `max_test_samples` allow you to truncate the number of examples.
They recieve digits digit format.

```
python run_pos.py \
 --model_name_or_path models/custom-model-demo.py \
 --output_dir /tmp/pos-exp-1 \
 --task_name pos \
 --dataset_script ontonotes_v4.py \
 --max_seq_length 63 \
 --per_device_train_batch_size 24 \
 --per_device_eval_batch_size 8 \
 --num_train_epochs 3 \
 --do_train \
 --do_eval \
 --do_predict \
 --max_train_samples 10000 \
 --max_val_samples 1000 \
 --max_test_samples 1000 \
 --logging_steps 20 
 --learning_rate 1e-2 \
```


