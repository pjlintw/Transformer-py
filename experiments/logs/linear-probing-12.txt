(fabian-pinjie) linus@linpinjies-MBP transformer-py %  python run_pos.py \
 --model_name_or_path models/linear-probing-bert.py \
 --output_dir /tmp/pos-exp-1 \
 --task_name pos \
 --dataset_script ontonotes_v4.py \
 --max_seq_length 63 \
 --per_device_train_batch_size 48 \
 --per_device_eval_batch_size 48 \
 --max_steps 120 \
 --do_train \
 --do_eval \
 --do_predict \
 --max_train_samples 10000 \
 --max_val_samples 300 \
 --max_test_samples 300 \
 --logging_first_step \
 --logging_steps 5 \
 --learning_rate 1e-2 \
 --evaluation_strategy steps \
 --eval_steps 10 \
 --to_layer 12
03/12/2021 12:32:04 - WARNING - datasets.builder -   Reusing dataset ontonotes4 (/Users/linus/.cache/huggingface/datasets/ontonotes4/ontonotes4/0.0.0/fe3b1d886449dd8016ef9f4c12917b1bd64f9515b0ea7b17482c2c844a16cc9b)
Creating custom model in models/linear-probing-bert.py
03/12/2021 12:33:35 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /Users/linus/.cache/huggingface/datasets/ontonotes4/ontonotes4/0.0.0/fe3b1d886449dd8016ef9f4c12917b1bd64f9515b0ea7b17482c2c844a16cc9b/cache-911efc813c174f5c.arrow
03/12/2021 12:33:36 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /Users/linus/.cache/huggingface/datasets/ontonotes4/ontonotes4/0.0.0/fe3b1d886449dd8016ef9f4c12917b1bd64f9515b0ea7b17482c2c844a16cc9b/cache-4dfe332d71fa74d4.arrow
03/12/2021 12:33:36 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /Users/linus/.cache/huggingface/datasets/ontonotes4/ontonotes4/0.0.0/fe3b1d886449dd8016ef9f4c12917b1bd64f9515b0ea7b17482c2c844a16cc9b/cache-ee73a6652a3faffd.arrow
03/12/2021 12:33:36 - WARNING - datasets.load -   Using the latest cached version of the module from /Users/linus/.cache/huggingface/modules/datasets_modules/metrics/seqeval/ec5b7242a8c40468d189ca0b2b10612578dbcad311b2a134c99e3ded58a0d6e3 (last modified on Thu Mar  4 21:53:16 2021) since it couldn't be found locally at seqeval/seqeval.py or remotely (ConnectionError).
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|                                                                                                         | 0/120 [00:00<?, ?it/s]/Users/linus/opt/anaconda3/envs/fabian-pinjie/lib/python3.8/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
{'loss': 3.8885, 'learning_rate': 0.009916666666666667, 'epoch': 0.0}                                                                 
{'loss': 2.1993, 'learning_rate': 0.009583333333333334, 'epoch': 0.02}                                                                
{'loss': 1.1227, 'learning_rate': 0.009166666666666667, 'epoch': 0.05}                                                                
{'eval_loss': 0.7177990078926086, 'eval_precision': 0.6209245910123578, 'eval_recall': 0.5608220906565564, 'eval_f1': 0.5756801354149643, 'eval_accuracy': 0.8588472265261853, 'eval_runtime': 42.0395, 'eval_samples_per_second': 7.136, 'epoch': 0.05}                    
{'loss': 0.67, 'learning_rate': 0.00875, 'epoch': 0.07}                                                                               
{'loss': 0.493, 'learning_rate': 0.008333333333333333, 'epoch': 0.1}                                                                  
{'eval_loss': 0.39731624722480774, 'eval_precision': 0.7597312528749546, 'eval_recall': 0.7349354925455092, 'eval_f1': 0.7395090965410489, 'eval_accuracy': 0.9099783080260304, 'eval_runtime': 40.3788, 'eval_samples_per_second': 7.43, 'epoch': 0.1}                     
{'loss': 0.4393, 'learning_rate': 0.007916666666666666, 'epoch': 0.12}                                                                
{'loss': 0.4007, 'learning_rate': 0.0075, 'epoch': 0.14}                                                                              
{'eval_loss': 0.31049224734306335, 'eval_precision': 0.8072807521197951, 'eval_recall': 0.7668214055976225, 'eval_f1': 0.7735561634015956, 'eval_accuracy': 0.9209792376820576, 'eval_runtime': 40.9011, 'eval_samples_per_second': 7.335, 'epoch': 0.14}                   
{'loss': 0.3573, 'learning_rate': 0.007083333333333334, 'epoch': 0.17}                                                                
{'loss': 0.3254, 'learning_rate': 0.006666666666666666, 'epoch': 0.19}                                                                
{'eval_loss': 0.27273640036582947, 'eval_precision': 0.8276755957127793, 'eval_recall': 0.8197949390986375, 'eval_f1': 0.8166201842941959, 'eval_accuracy': 0.9281066005577936, 'eval_runtime': 40.2573, 'eval_samples_per_second': 7.452, 'epoch': 0.19}                   
{'loss': 0.298, 'learning_rate': 0.00625, 'epoch': 0.22}                                                                              
{'loss': 0.32, 'learning_rate': 0.005833333333333334, 'epoch': 0.24}                                                                  
{'eval_loss': 0.2448330670595169, 'eval_precision': 0.8542243991242205, 'eval_recall': 0.838894988252708, 'eval_f1': 0.8384967550007033, 'eval_accuracy': 0.9370932754880694, 'eval_runtime': 41.3068, 'eval_samples_per_second': 7.263, 'epoch': 0.24}                     
{'loss': 0.3065, 'learning_rate': 0.005416666666666666, 'epoch': 0.26}                                                                
{'loss': 0.2944, 'learning_rate': 0.005, 'epoch': 0.29}                                                                               
{'eval_loss': 0.2304723858833313, 'eval_precision': 0.8918376602675153, 'eval_recall': 0.8361115597138666, 'eval_f1': 0.8538626869942005, 'eval_accuracy': 0.9384877595289742, 'eval_runtime': 41.4859, 'eval_samples_per_second': 7.231, 'epoch': 0.29}                    
{'loss': 0.2908, 'learning_rate': 0.004583333333333333, 'epoch': 0.31}                                                                
{'loss': 0.2725, 'learning_rate': 0.004166666666666667, 'epoch': 0.33}                                                                
{'eval_loss': 0.21996277570724487, 'eval_precision': 0.8963692706390191, 'eval_recall': 0.8575719621738794, 'eval_f1': 0.8717809674750978, 'eval_accuracy': 0.943910753021382, 'eval_runtime': 42.8116, 'eval_samples_per_second': 7.007, 'epoch': 0.33}                    
{'loss': 0.275, 'learning_rate': 0.00375, 'epoch': 0.36}                                                                              
{'loss': 0.2652, 'learning_rate': 0.003333333333333333, 'epoch': 0.38}                                                                
{'eval_loss': 0.2112412452697754, 'eval_precision': 0.9154868179293125, 'eval_recall': 0.872299825007, 'eval_f1': 0.8868136335096845, 'eval_accuracy': 0.9443755810350171, 'eval_runtime': 41.1789, 'eval_samples_per_second': 7.285, 'epoch': 0.38}                        
{'loss': 0.2655, 'learning_rate': 0.002916666666666667, 'epoch': 0.41}                                                                
{'loss': 0.2357, 'learning_rate': 0.0025, 'epoch': 0.43}                                                                              
{'eval_loss': 0.20764383673667908, 'eval_precision': 0.9104959115362294, 'eval_recall': 0.8858597491095368, 'eval_f1': 0.8927383342131013, 'eval_accuracy': 0.943910753021382, 'eval_runtime': 40.8748, 'eval_samples_per_second': 7.339, 'epoch': 0.43}                    
{'loss': 0.2744, 'learning_rate': 0.0020833333333333333, 'epoch': 0.45}                                                               
{'loss': 0.2619, 'learning_rate': 0.0016666666666666666, 'epoch': 0.48}                                                               
{'eval_loss': 0.2046026885509491, 'eval_precision': 0.9123419828280698, 'eval_recall': 0.8939959313582213, 'eval_f1': 0.8964549999291956, 'eval_accuracy': 0.9466997211031918, 'eval_runtime': 39.3069, 'eval_samples_per_second': 7.632, 'epoch': 0.48}                    
{'loss': 0.2518, 'learning_rate': 0.00125, 'epoch': 0.5}                                                                              
{'loss': 0.2684, 'learning_rate': 0.0008333333333333333, 'epoch': 0.53}                                                               
{'eval_loss': 0.20081184804439545, 'eval_precision': 0.9124060210237706, 'eval_recall': 0.8940413044790134, 'eval_f1': 0.8965611259798216, 'eval_accuracy': 0.9470096064456152, 'eval_runtime': 39.0893, 'eval_samples_per_second': 7.675, 'epoch': 0.53}                   
{'loss': 0.2376, 'learning_rate': 0.00041666666666666664, 'epoch': 0.55}                                                              
{'loss': 0.2401, 'learning_rate': 0.0, 'epoch': 0.57}                                                                                 
{'eval_loss': 0.2000274658203125, 'eval_precision': 0.915253486135521, 'eval_recall': 0.8947985702261079, 'eval_f1': 0.8993763722644461, 'eval_accuracy': 0.9473194917880384, 'eval_runtime': 39.4458, 'eval_samples_per_second': 7.605, 'epoch': 0.57}                     
{'train_runtime': 1295.8515, 'train_samples_per_second': 0.093, 'epoch': 0.57}                                                        
100%|███████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [21:35<00:00, 10.80s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:33<00:00,  4.84s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:33<00:00,  4.32s/it]Test metrics {'eval_loss': 0.2000274658203125, 'eval_precision': 0.915253486135521, 'eval_recall': 0.8947985702261079, 'eval_f1': 0.8993763722644461, 'eval_accuracy': 0.9473194917880384, 'eval_runtime': 39.8174, 'eval_samples_per_second': 7.534}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:33<00:00,  4.79s/it]
