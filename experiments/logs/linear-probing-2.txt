(fabian-pinjie) linus@linpinjies-MBP transformer-py %  python run_pos.py \
 --model_name_or_path models/linear-probing-bert.py \
 --output_dir /tmp/pos-exp-1 \
 --task_name pos \
 --dataset_script ontonotes_v4.py \
 --max_seq_length 63 \
 --per_device_train_batch_size 48 \
 --per_device_eval_batch_size 48 \
 --max_steps 120 \
 --do_train \
 --do_eval \
 --do_predict \
 --max_train_samples 10000 \
 --max_val_samples 300 \
 --max_test_samples 300 \
 --logging_first_step \
 --logging_steps 5 \
 --learning_rate 1e-2 \
 --evaluation_strategy steps \
 --eval_steps 10 \
 --to_layer 2
03/11/2021 22:33:56 - WARNING - datasets.builder -   Reusing dataset ontonotes4 (/Users/linus/.cache/huggingface/datasets/ontonotes4/ontonotes4/0.0.0/fe3b1d886449dd8016ef9f4c12917b1bd64f9515b0ea7b17482c2c844a16cc9b)
Creating custom model in models/linear-probing-bert.py
03/11/2021 22:34:07 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /Users/linus/.cache/huggingface/datasets/ontonotes4/ontonotes4/0.0.0/fe3b1d886449dd8016ef9f4c12917b1bd64f9515b0ea7b17482c2c844a16cc9b/cache-911efc813c174f5c.arrow
03/11/2021 22:34:08 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /Users/linus/.cache/huggingface/datasets/ontonotes4/ontonotes4/0.0.0/fe3b1d886449dd8016ef9f4c12917b1bd64f9515b0ea7b17482c2c844a16cc9b/cache-4dfe332d71fa74d4.arrow
03/11/2021 22:34:08 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /Users/linus/.cache/huggingface/datasets/ontonotes4/ontonotes4/0.0.0/fe3b1d886449dd8016ef9f4c12917b1bd64f9515b0ea7b17482c2c844a16cc9b/cache-ee73a6652a3faffd.arrow
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|                                                           | 0/120 [00:00<?, ?it/s]/Users/linus/opt/anaconda3/envs/fabian-pinjie/lib/python3.8/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
{'loss': 3.9425, 'learning_rate': 0.009916666666666667, 'epoch': 0.0}                   
{'loss': 1.2421, 'learning_rate': 0.009583333333333334, 'epoch': 0.02}                  
{'loss': 0.4336, 'learning_rate': 0.009166666666666667, 'epoch': 0.05}                  
{'eval_loss': 0.3299397826194763, 'eval_precision': 0.7474673681016286, 'eval_recall': 0.7276031554979069, 'eval_f1': 0.7184448203635497, 'eval_accuracy': 0.9070343972730089, 'eval_runtime': 37.9032, 'eval_samples_per_second': 7.915, 'epoch': 0.05}
{'loss': 0.3065, 'learning_rate': 0.00875, 'epoch': 0.07}                               
{'loss': 0.2891, 'learning_rate': 0.008333333333333333, 'epoch': 0.1}                   
{'eval_loss': 0.2641194462776184, 'eval_precision': 0.8226354182089721, 'eval_recall': 0.8431339370798456, 'eval_f1': 0.8234296459701783, 'eval_accuracy': 0.9251626898047722, 'eval_runtime': 42.1609, 'eval_samples_per_second': 7.116, 'epoch': 0.1}
{'loss': 0.3222, 'learning_rate': 0.007916666666666666, 'epoch': 0.12}                  
{'loss': 0.2883, 'learning_rate': 0.0075, 'epoch': 0.14}                                
{'eval_loss': 0.2217162549495697, 'eval_precision': 0.8351514392145949, 'eval_recall': 0.8407654476131986, 'eval_f1': 0.8341341702747035, 'eval_accuracy': 0.938177874186551, 'eval_runtime': 39.7981, 'eval_samples_per_second': 7.538, 'epoch': 0.14}
{'loss': 0.2733, 'learning_rate': 0.007083333333333334, 'epoch': 0.17}                  
{'loss': 0.2396, 'learning_rate': 0.006666666666666666, 'epoch': 0.19}                  
{'eval_loss': 0.20417721569538116, 'eval_precision': 0.86852063530132, 'eval_recall': 0.8655346706181465, 'eval_f1': 0.860825811459985, 'eval_accuracy': 0.9422063836380539, 'eval_runtime': 39.2335, 'eval_samples_per_second': 7.647, 'epoch': 0.19}
{'loss': 0.2524, 'learning_rate': 0.00625, 'epoch': 0.22}                               
{'loss': 0.277, 'learning_rate': 0.005833333333333334, 'epoch': 0.24}                   
{'eval_loss': 0.19181212782859802, 'eval_precision': 0.8799327300550946, 'eval_recall': 0.8808973034817557, 'eval_f1': 0.875414325997994, 'eval_accuracy': 0.944840409048652, 'eval_runtime': 39.6642, 'eval_samples_per_second': 7.563, 'epoch': 0.24}
{'loss': 0.2617, 'learning_rate': 0.005416666666666666, 'epoch': 0.26}                  
{'loss': 0.2416, 'learning_rate': 0.005, 'epoch': 0.29}                                 
{'eval_loss': 0.18204325437545776, 'eval_precision': 0.9078912140212305, 'eval_recall': 0.9088708126944997, 'eval_f1': 0.9054832886367083, 'eval_accuracy': 0.9451502943910753, 'eval_runtime': 39.3386, 'eval_samples_per_second': 7.626, 'epoch': 0.29}
{'loss': 0.2333, 'learning_rate': 0.004583333333333333, 'epoch': 0.31}                  
{'loss': 0.226, 'learning_rate': 0.004166666666666667, 'epoch': 0.33}                   
{'eval_loss': 0.17586453258991241, 'eval_precision': 0.9250669209507236, 'eval_recall': 0.9048077252269023, 'eval_f1': 0.9100120321117282, 'eval_accuracy': 0.9470096064456152, 'eval_runtime': 39.2164, 'eval_samples_per_second': 7.65, 'epoch': 0.33}
{'loss': 0.2319, 'learning_rate': 0.00375, 'epoch': 0.36}                               
{'loss': 0.2214, 'learning_rate': 0.003333333333333333, 'epoch': 0.38}                  
{'eval_loss': 0.16973699629306793, 'eval_precision': 0.9263219144491847, 'eval_recall': 0.9167933870923698, 'eval_f1': 0.9179987415759404, 'eval_accuracy': 0.9505732878834832, 'eval_runtime': 37.973, 'eval_samples_per_second': 7.9, 'epoch': 0.38}
{'loss': 0.2217, 'learning_rate': 0.002916666666666667, 'epoch': 0.41}                  
{'loss': 0.1984, 'learning_rate': 0.0025, 'epoch': 0.43}                                
{'eval_loss': 0.16510526835918427, 'eval_precision': 0.9163325948661906, 'eval_recall': 0.9104360951326883, 'eval_f1': 0.9115822112583623, 'eval_accuracy': 0.9508831732259064, 'eval_runtime': 39.3036, 'eval_samples_per_second': 7.633, 'epoch': 0.43}
{'loss': 0.2214, 'learning_rate': 0.0020833333333333333, 'epoch': 0.45}                 
{'loss': 0.2236, 'learning_rate': 0.0016666666666666666, 'epoch': 0.48}                 
{'eval_loss': 0.16344080865383148, 'eval_precision': 0.9154449613117899, 'eval_recall': 0.9166153969400079, 'eval_f1': 0.913468592035743, 'eval_accuracy': 0.951967771924388, 'eval_runtime': 37.9232, 'eval_samples_per_second': 7.911, 'epoch': 0.48}
{'loss': 0.1888, 'learning_rate': 0.00125, 'epoch': 0.5}                                
{'loss': 0.2249, 'learning_rate': 0.0008333333333333333, 'epoch': 0.53}                 
{'eval_loss': 0.15930195152759552, 'eval_precision': 0.9214032718415791, 'eval_recall': 0.9212595036814093, 'eval_f1': 0.9197527312548546, 'eval_accuracy': 0.9525875426092346, 'eval_runtime': 40.2321, 'eval_samples_per_second': 7.457, 'epoch': 0.53}
{'loss': 0.2005, 'learning_rate': 0.00041666666666666664, 'epoch': 0.55}                
{'loss': 0.1967, 'learning_rate': 0.0, 'epoch': 0.57}                                   
{'eval_loss': 0.1577395796775818, 'eval_precision': 0.9229426736695667, 'eval_recall': 0.921510557121403, 'eval_f1': 0.9208680034543468, 'eval_accuracy': 0.9525875426092346, 'eval_runtime': 39.1427, 'eval_samples_per_second': 7.664, 'epoch': 0.57}
{'train_runtime': 1245.5239, 'train_samples_per_second': 0.096, 'epoch': 0.57}          
100%|█████████████████████████████████████████████████| 120/120 [20:45<00:00, 10.38s/it]
100%|█████████████████████████████████████████████████████| 7/7 [00:32<00:00,  4.70s/it]
100%|█████████████████████████████████████████████████████| 7/7 [00:32<00:00,  4.33s/it]Test metrics {'eval_loss': 0.1577395796775818, 'eval_precision': 0.9229426736695667, 'eval_recall': 0.921510557121403, 'eval_f1': 0.9208680034543468, 'eval_accuracy': 0.9525875426092346, 'eval_runtime': 39.2079, 'eval_samples_per_second': 7.652}
100%|█████████████████████████████████████████████████████| 7/7 [00:32<00:00,  4.70s/it]
