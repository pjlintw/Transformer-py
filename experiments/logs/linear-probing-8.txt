(fabian-pinjie) linus@linpinjies-MBP transformer-py %  python run_pos.py \
 --model_name_or_path models/linear-probing-bert.py \
 --output_dir /tmp/pos-exp-1 \
 --task_name pos \
 --dataset_script ontonotes_v4.py \
 --max_seq_length 63 \
 --per_device_train_batch_size 48 \
 --per_device_eval_batch_size 48 \
 --max_steps 120 \
 --do_train \
 --do_eval \
 --do_predict \
 --max_train_samples 10000 \
 --max_val_samples 300 \
 --max_test_samples 300 \
 --logging_first_step \
 --logging_steps 5 \
 --learning_rate 1e-2 \
 --evaluation_strategy steps \
 --eval_steps 10 \
 --to_layer 8
03/12/2021 00:59:39 - WARNING - datasets.builder -   Reusing dataset ontonotes4 (/Users/linus/.cache/huggingface/datasets/ontonotes4/ontonotes4/0.0.0/fe3b1d886449dd8016ef9f4c12917b1bd64f9515b0ea7b17482c2c844a16cc9b)
Creating custom model in models/linear-probing-bert.py
03/12/2021 00:59:51 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /Users/linus/.cache/huggingface/datasets/ontonotes4/ontonotes4/0.0.0/fe3b1d886449dd8016ef9f4c12917b1bd64f9515b0ea7b17482c2c844a16cc9b/cache-911efc813c174f5c.arrow
03/12/2021 00:59:51 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /Users/linus/.cache/huggingface/datasets/ontonotes4/ontonotes4/0.0.0/fe3b1d886449dd8016ef9f4c12917b1bd64f9515b0ea7b17482c2c844a16cc9b/cache-4dfe332d71fa74d4.arrow
03/12/2021 00:59:51 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /Users/linus/.cache/huggingface/datasets/ontonotes4/ontonotes4/0.0.0/fe3b1d886449dd8016ef9f4c12917b1bd64f9515b0ea7b17482c2c844a16cc9b/cache-ee73a6652a3faffd.arrow
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|                                                                                             | 0/120 [00:00<?, ?it/s]/Users/linus/opt/anaconda3/envs/fabian-pinjie/lib/python3.8/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
{'loss': 3.9674, 'learning_rate': 0.009916666666666667, 'epoch': 0.0}                                                     
{'loss': 1.4294, 'learning_rate': 0.009583333333333334, 'epoch': 0.02}                                                    
{'loss': 0.47, 'learning_rate': 0.009166666666666667, 'epoch': 0.05}                                                      
{'eval_loss': 0.3083679974079132, 'eval_precision': 0.7505637644462259, 'eval_recall': 0.7572933365321654, 'eval_f1': 0.7489872069492064, 'eval_accuracy': 0.9143167028199566, 'eval_runtime': 42.3416, 'eval_samples_per_second': 7.085, 'epoch': 0.05}
{'loss': 0.2903, 'learning_rate': 0.00875, 'epoch': 0.07}                                                                 
{'loss': 0.2313, 'learning_rate': 0.008333333333333333, 'epoch': 0.1}                                                     
{'eval_loss': 0.2183067351579666, 'eval_precision': 0.8406191073172431, 'eval_recall': 0.8266788259017588, 'eval_f1': 0.8194625313154498, 'eval_accuracy': 0.9383328168577626, 'eval_runtime': 41.0972, 'eval_samples_per_second': 7.3, 'epoch': 0.1}
{'loss': 0.2588, 'learning_rate': 0.007916666666666666, 'epoch': 0.12}                                                    
{'loss': 0.2445, 'learning_rate': 0.0075, 'epoch': 0.14}                                                                  
{'eval_loss': 0.1802240014076233, 'eval_precision': 0.8671675269458292, 'eval_recall': 0.8457891738339849, 'eval_f1': 0.8465119848351234, 'eval_accuracy': 0.9474744344592501, 'eval_runtime': 40.8446, 'eval_samples_per_second': 7.345, 'epoch': 0.14}
{'loss': 0.2298, 'learning_rate': 0.007083333333333334, 'epoch': 0.17}                                                    
{'loss': 0.209, 'learning_rate': 0.006666666666666666, 'epoch': 0.19}                                                     
{'eval_loss': 0.16593849658966064, 'eval_precision': 0.883487239320219, 'eval_recall': 0.8863956702250719, 'eval_f1': 0.8763182621587748, 'eval_accuracy': 0.952432599938023, 'eval_runtime': 39.7562, 'eval_samples_per_second': 7.546, 'epoch': 0.19}
{'loss': 0.1973, 'learning_rate': 0.00625, 'epoch': 0.22}                                                                 
{'loss': 0.2107, 'learning_rate': 0.005833333333333334, 'epoch': 0.24}                                                    
{'eval_loss': 0.15600430965423584, 'eval_precision': 0.9184079707140835, 'eval_recall': 0.912781977316613, 'eval_f1': 0.9121032076029615, 'eval_accuracy': 0.9549116826774093, 'eval_runtime': 39.6069, 'eval_samples_per_second': 7.574, 'epoch': 0.24}
{'loss': 0.2111, 'learning_rate': 0.005416666666666666, 'epoch': 0.26}                                                    
{'loss': 0.1944, 'learning_rate': 0.005, 'epoch': 0.29}                                                                   
{'eval_loss': 0.14225894212722778, 'eval_precision': 0.9425728256301585, 'eval_recall': 0.9455916143262759, 'eval_f1': 0.9422181101308588, 'eval_accuracy': 0.9600247908273939, 'eval_runtime': 39.1892, 'eval_samples_per_second': 7.655, 'epoch': 0.29}
{'loss': 0.1902, 'learning_rate': 0.004583333333333333, 'epoch': 0.31}                                                    
{'loss': 0.1743, 'learning_rate': 0.004166666666666667, 'epoch': 0.33}                                                    
{'eval_loss': 0.13904637098312378, 'eval_precision': 0.9430174605721735, 'eval_recall': 0.9321990352516113, 'eval_f1': 0.9314274168950154, 'eval_accuracy': 0.9607995041834522, 'eval_runtime': 42.4508, 'eval_samples_per_second': 7.067, 'epoch': 0.33}
{'loss': 0.175, 'learning_rate': 0.00375, 'epoch': 0.36}                                                                  
{'loss': 0.1762, 'learning_rate': 0.003333333333333333, 'epoch': 0.38}                                                    
{'eval_loss': 0.13214696943759918, 'eval_precision': 0.9419377946427318, 'eval_recall': 0.9497948035361137, 'eval_f1': 0.9434156541246614, 'eval_accuracy': 0.9631236442516269, 'eval_runtime': 39.447, 'eval_samples_per_second': 7.605, 'epoch': 0.38}
{'loss': 0.164, 'learning_rate': 0.002916666666666667, 'epoch': 0.41}                                                     
{'loss': 0.1607, 'learning_rate': 0.0025, 'epoch': 0.43}                                                                  
{'eval_loss': 0.12762178480625153, 'eval_precision': 0.9475769524795561, 'eval_recall': 0.9448780118025853, 'eval_f1': 0.9451715753102753, 'eval_accuracy': 0.9640533002788968, 'eval_runtime': 40.4865, 'eval_samples_per_second': 7.41, 'epoch': 0.43}
{'loss': 0.1794, 'learning_rate': 0.0020833333333333333, 'epoch': 0.45}                                                   
{'loss': 0.1687, 'learning_rate': 0.0016666666666666666, 'epoch': 0.48}                                                   
{'eval_loss': 0.12786169350147247, 'eval_precision': 0.9273736223295346, 'eval_recall': 0.9208352713529839, 'eval_f1': 0.9226468703776728, 'eval_accuracy': 0.9638983576076852, 'eval_runtime': 40.4859, 'eval_samples_per_second': 7.41, 'epoch': 0.48}
{'loss': 0.1547, 'learning_rate': 0.00125, 'epoch': 0.5}                                                                  
{'loss': 0.1635, 'learning_rate': 0.0008333333333333333, 'epoch': 0.53}                                                   
{'eval_loss': 0.1253814846277237, 'eval_precision': 0.9332360834748329, 'eval_recall': 0.9319185647827638, 'eval_f1': 0.9303161969508779, 'eval_accuracy': 0.9656027269910133, 'eval_runtime': 39.0721, 'eval_samples_per_second': 7.678, 'epoch': 0.53}
{'loss': 0.1467, 'learning_rate': 0.00041666666666666664, 'epoch': 0.55}                                                  
{'loss': 0.148, 'learning_rate': 0.0, 'epoch': 0.57}                                                                      
{'eval_loss': 0.12439776211977005, 'eval_precision': 0.9334377681078941, 'eval_recall': 0.9322348488684277, 'eval_f1': 0.9305723022583188, 'eval_accuracy': 0.9662224976758599, 'eval_runtime': 39.114, 'eval_samples_per_second': 7.67, 'epoch': 0.57}
{'train_runtime': 1262.3389, 'train_samples_per_second': 0.095, 'epoch': 0.57}                                            
100%|███████████████████████████████████████████████████████████████████████████████████| 120/120 [21:02<00:00, 10.52s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:35<00:00,  5.11s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:34<00:00,  4.49s/it]Test metrics {'eval_loss': 0.12439776211977005, 'eval_precision': 0.9334377681078941, 'eval_recall': 0.9322348488684277, 'eval_f1': 0.9305723022583188, 'eval_accuracy': 0.9662224976758599, 'eval_runtime': 40.7988, 'eval_samples_per_second': 7.353}
100%|███████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:34<00:00,  4.96s/it]